
```
docker inspect --format "{{ .Volumes }}" container
```


# 容器
## 背景
- 容器技术的兴起源于 **PaaS** 技术的普及
- Docker 公司发布对docker项目具有里程碑式意义
- docker项目通过“容器镜像”，解决了**应用打包**的根本难题
- 容器本身没有价值，有价值的是**容器编排**
- 容器是一个**单进程** ------ 只有一个进程是可控的

## 容器是一种沙盒技术
### 沙盒边界的定义
- 代码二进制+数据二进制 -> 程序 -> **代码的可执行镜像**
	-> **进程的静态表现**
- 程序运行 -> 内存里的数据+寄存器的值+堆栈指令+被打开的文件+各种设备的状态信息的**集合** 
	-> **进程的动态表现**
- 容器的核心技术：通过约束和修改进程的动态表现，从而为其创造出**边界**

- Cgroups用来约束进程的动态表现
- Namespace用来修改进程的视图(障眼法)

-> **容器只是运行在宿主机上的一个特殊进程**

### 隔离与限制的理解
进一步理解 **容器只是运行在宿主机上的一个特殊进程**
- 隔离
	- 容器的软件实体是依赖于宿主机的
	- Linux内核中有很多资源和对象是不能被namespace隔离化的，最典型的例子就是时间
	- 共享宿主机内核使得容器暴露出的攻击面大，一般不会把容器服务直接暴露在公网中
	- [Namespace](https://lwn.net/Articles/531114/)
		- 容器使用的六种Namespace: Mount Namespace(挂载点), UTS Namespace(主机名与域名), IPC Namespace(信号量，消息队列和共享内存), PID Namespace, Network Namespace, User Namespace
		- 对应六个系统调用参数：CLONE_NEWNS, CLONE_NEWUTS, CLONE_NEWIPC, CLONE_NEWPID, CLONE_NEWNET, CLONE_NEWUSER
		- Mount Namespace改变的是容器进程对文件系统**挂载点**的认知，只有进行挂载操作之后，才能改变容器进程对文件系统的视图认知
- 限制（cgroups）
	- 早期被cgroup限制的进程组也被称为容器 
	- cgroups可以做：
		- 资源限制
		- 进程优先级设置和审计
		- 进程挂起和恢复
	- cgroups给用户暴露出来的操作接口是通过文件系统，是子系统目录加一组资源文件的组合
	- 对于Docker等容器项目只需要在每个子系统下，为每个容器创建一个资源控制组，然后在启动容器进程后，把这个进程加入该资源控制组

### 容器镜像
- 即使开启了Mount Namespace，容器看到的文件系统和宿主机一致
- 挂载在容器根目录上，用来为容器进程隔离后执行环境的文件系统，就是所谓的容器镜像，他有一个更专业的名字叫“rootfs”，根文件系统。常见的根文件系统通常包括/bin, /etc, /proc等
- rootfs只包括操作系统的外壳，不包括操作系统的灵魂，即内核，容器与宿主机共享内核，内核参数是全剧参数，牵一发而动全身
- 由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，它就意味着，应用以及运行所需要的依赖全部封装在一起，保证了**一致性**。
- 如果每次修改image都重新制作一次rootfs，会影响合作开发流程，因为不同应用的需求不一样。docker引入了增量修改的概念，引入了**层**的概念，用户的每一步操作，都是一个**层**，一个**增量rootfs**，基于联合文件系统操作实现的
- copy-on-write: 修改只读层的文件内容时，先复制到读写层后修改
whiteout: 删除只读层里的文件x时，会在读写层创建.wh.x。联合挂载的时候，x文件就会被.wh.x隐藏遮盖
init层: 只对当前容器有效的设置，且只读，比如hostname

### 创建容器
1. 启用Linux Namespace设置
2. 设置指定Cgroups参数
3. 切换进程的根目录

## Docker子命令
![docker命令](https://img-blog.csdn.net/20180629115036543?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Fubm90YXRpb25feWFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

docker commit: 发生在宿主机namespace的

docker exec: 启动进程并进入容器进程所在的namespace中

volume: 在启用namespace之后，在chroot之前，将hostpath挂载到容器可读写层目录上；由于此时已经启动mount namespace，在宿主机上看到的该目录为空，因而commit也不会提交该目录下的内容
### 实验
#### Cgroups
1. 基本操作指示
	- 操作接口的文件系统位置: ```/sys/fs/cgroups```
	- 查看cgroup操作接口: ```mount -t cgroup```, 每个目录都是一个子系统
		```
		nina@nina-VirtualBox:~$ mount -t cgroup
		cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)
		cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
		cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
		cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
		cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
		cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
		cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
		cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
		cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
		cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
		cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
		cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)
		```
		
	- 查看每个资源可以被限制的方法: ```ls /sys/fs/cgroup/xxx```
		```
		nina@nina-VirtualBox:~$ ls /sys/fs/cgroup/memory
		cgroup.clone_children       memory.kmem.max_usage_in_bytes      memory.max_usage_in_bytes        memory.usage_in_bytes
		cgroup.event_control        memory.kmem.slabinfo                memory.move_charge_at_immigrate  memory.use_hierarchy
		cgroup.procs                memory.kmem.tcp.failcnt             memory.numa_stat                 notify_on_release
		cgroup.sane_behavior        memory.kmem.tcp.limit_in_bytes      memory.oom_control               release_agent
		memory.failcnt              memory.kmem.tcp.max_usage_in_bytes  memory.pressure_level            tasks
		memory.force_empty          memory.kmem.tcp.usage_in_bytes      memory.soft_limit_in_bytes
		memory.kmem.failcnt         memory.kmem.usage_in_bytes          memory.stat
		memory.kmem.limit_in_bytes  memory.limit_in_bytes               memory.swappiness
		```
	- Cgroup每一项子系统都有各自的功能
		- blkio限制设备I/O
		- memory限制内存资源
		- cpuset限制cpu核和对应的内存节点
		- cpu限制cpu资源
		- ...
	- 本实验基础: ```/sys/fs/cgroup/cpu/cfs_period```和```/sys/fs/cgroup/cfs_quota```可以限制进程在长度为cfs_period的时间段内，只能被分配到总量为cfs_quota的cpu时间

2. 限制一个进程的资源
	- 创建一个死循环进程，不加以限制
	  
	  创建进程
	  ```
	  nina@nina-VirtualBox:~$ while : ; do : ; done &
	  [1] 29536
	  ```
	  
	  查看进程资源
	  ```
	  nina@nina-VirtualBox:~$ top
	  top - 17:13:16 up 13 min,  2 users,  load average: 0.84, 0.77, 0.57
	  任务: 159 total,   2 running, 122 sleeping,   0 stopped,   0 zombie
	  %Cpu(s): 99.6 us,  0.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.4 si,  0.0 st
	  KiB Mem :  4039508 total,   823168 free,   449052 used,  2767288 buff/cache
	  KiB Swap:   483800 total,   483800 free,        0 used.  3301680 avail Mem
	  进程 USER      PR  NI    VIRT    RES    SHR � %CPU %MEM     TIME+ COMMAND
	  29536 nina      20   0   31412   2780   1340 R 99.3  0.1   1:26.09 bash
	  ```
	 - 创建一个新的cpu资源控制组
	   
	   创建cpu控制组
		```
		nina@nina-VirtualBox:/sys/fs/cgroup/cpu$ sudo mkdir cgroup_test
		[sudo] nina 的密码：
		nina@nina-VirtualBox:/sys/fs/cgroup/cpu$ ls cgroup_test/
		cgroup.clone_children  cpu.shares     cpuacct.usage_all          cpuacct.usage_sys
		cgroup.procs           cpu.stat       cpuacct.usage_percpu       cpuacct.usage_user
		cpu.cfs_period_us      cpuacct.stat   cpuacct.usage_percpu_sys   notify_on_release
		cpu.cfs_quota_us       cpuacct.usage  cpuacct.usage_percpu_user  tasks
		```

		查看cfs_period和cfs_quota
		```
		nina@nina-VirtualBox:/sys/fs/cgroup/cpu$ cat cgroup_test/cpu.cfs_period_us
		100000
		nina@nina-VirtualBox:/sys/fs/cgroup/cpu$ cat cgroup_test/cpu.cfs_quota_us
		-1
		```
	  - 修改控制组的cfs_period和cfs_quota的值
	   
	    向cgroup_test控制组的cfs_quota文件中写入20ms(20000um), 这意味着被该进程组限制的进程，每100ms的cpu运行时间只能使用20ms，cpu带宽为20%
	    ```echo 20000 > cgroup_test/cpu.cfs_quota_us```
	    
	    把死循环进程id写入```cgroup_test/task```
	    ```echo 29536 > cgroup_test/tasks```

		查看进程状态
		```
		nina@nina-VirtualBox:~$ top
		top - 17:37:57 up 38 min,  2 users,  load average: 0.61, 0.80, 0.67
		任务: 161 total,   2 running, 124 sleeping,   0 stopped,   0 zombie
		%Cpu(s): 16.7 us,  0.3 sy,  0.0 ni, 82.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
		KiB Mem :  4039508 total,   829832 free,   440136 used,  2769540 buff/cache
		KiB Swap:   483800 total,   483800 free,        0 used.  3310592 avail Mem
		进程 USER      PR  NI    VIRT    RES    SHR � %CPU %MEM     TIME+ COMMAND
		29536 nina      20   0   31412   2648   1208 R 20.5  0.1   8:51.85 bash
		```
3. 创建一个docker容器
	创建容器并设置cfs-period和cfs-quota
	```
	docker run --rm -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash
	```
	查看容器id
	```
	nina@nina-VirtualBox:~$ docker ps -a
	CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES
	a51bbb4e99a0        ubuntu              "/bin/bash"         8 seconds ago       Up 6 seconds                                    affectionate_cori
	```
	查看对应的cgroup
	```
	nina@nina-VirtualBox:/sys/fs/cgroup/cpu/docker/a51bbb4e99a0bc230452b6b8bf463a0bd241c6c0cf56870f76c86708b4a308f4$ cat cpu.cfs_period_us
	100000
	nina@nina-VirtualBox:/sys/fs/cgroup/cpu/docker/a51bbb4e99a0bc230452b6b8bf463a0bd241c6c0cf56870f76c86708b4a308f4$ cat cpu.cfs_quota_us
	20000
	```
	问题:
	宿主机的资源
	```
	nina@nina-VirtualBox:~$ top
	top - 22:51:13 up  4:07,  3 users,  load average: 0.04, 0.01, 0.00
	任务: 218 total,   1 running, 184 sleeping,   0 stopped,   0 zombie
	%Cpu(s):  1.3 us,  0.7 sy,  0.0 ni, 98.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
	KiB Mem :  4037392 total,  1005316 free,  1184284 used,  1847792 buff/cache
	KiB Swap:   483800 total,   483800 free,        0 used.  2604064 avail Mem
	```
	容器中的资源
	```
	root@a51bbb4e99a0:/#  top
	top - 14:52:33 up  4:09,  0 users,  load average: 0.04, 0.02, 0.00
	Tasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie
	%Cpu(s):  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
	KiB Mem :  4037392 total,  1005316 free,  1184276 used,  1847800 buff/cache
	KiB Swap:   483800 total,   483800 free,        0 used.  2604064 avail Mem
	```
	容器中内存的total和宿主机一致，这会带来一些问题。比如，给容器分配4G内存，容器跑的应用使用默认参数，此时默认参数会根据容器中内存的大小（宿主机内存大小）分配默认内存，那么此时默认值可能是大于4G，从而造成OOM
	    
#### Namespace
Namespace的API包括clone(), setns(), unshare()
- clone在创建新进程的同时创建新的namespace
- setns加入一个已经存在的namespace
- unshare会在原先的进程上进行namespace隔离

```
#define _GNU_SOURCE
#include <sys/types.h>
#include <sys/wait.h>
#include <stdio.h>
#include <unistd.h>
#include <signal.h>
#include <sched.h>

#define STACK_SIZE (1024*1024)

static char child_stack[STACK_SIZE];
char* const child_args[] = {
        "/bin/bash",
        NULL
};

int child_main() {
        printf("in child process, process id is %d\n", getpid());
        execv(child_args[0], child_args);
        printf("execv replaced current process, this will not be run!");
        return 1;
}

int main() {
        printf("Parent process is starting, process id is %d\n", getpid());
        int child_pid = clone(child_main, child_stack+STACK_SIZE, SIGCHLD, NULL);
        waitpid(child_pid, NULL, 0); // wait for child process exiting
        printf("Child process exited, current is parent process, process id is %d\n", getpid());
        return 0;
}
```

#### 文件系统实验
chroot filesystem command

mount -t aufs -o dirs=./d_A:./d_B none ./d_C

docker image inspect ubuntu:latest

cd /var/lib/docker/overlay2

docker inspect --format '{{ .State.Pid }}' container_id



# Kubenetes
## Kubenetes关键词
- 集群管理
- Golang项目
- 调度
	- 过去很多集群管理项目(Swarm, mesos, yarn等)所擅长的就是把一个容器按照某种规则，放置在某个最佳节点上运行起来
- 编排
	- kubenetes是按照用户的意愿和整个系统的规则，完全自动化的处理好容器之间的关系
- 

## Kubenetes组件
Kubelete: 
Controller-Manager:
Scheduler:
Etcd:




## yaml文件
- 大小写敏感
- 缩进表示层级关系
- 在Kubenetes中，有两种结构： Maps和Lists
### Maps
key: value 键值对信息
### Lists
列表数组，'-'开头
### 声明式API
```
kubectl create xxx.yaml
kubectl replace xxx.yaml
```
上面是使用新的API对象替换原有的API对象， 命令式配置文件操作
```
kubectl apply xxx.yaml
```
这个才属于声明式API，执行了对原有API的PATCH操作
有什么区别呢：
- kube-apiserver在相应命令式请求的时候，一次只能处理一个写请求，否则会有可能产生冲突，而对于声明式请求，一次能处理多个写请求，而且具备merge能力
## pod
- 容器本质是进程，pod是一组容器(进程)
	- pod只是一个逻辑概念，kubenetes真正操作的还是宿主机的namespace和cgroup
	- 可以把pod看作传统环境里的虚拟机，容器看作运行在这个机器里面的用户用户程序
- pod里面所有的容器都共享同一个network namespace，并且可以声明共享同一个volume 
	- `docker run --net=B --volumes-from=B --name=A image-A` 这样也可以实现共享，但是B一定先起起来，A和B之间是拓扑关系，不是对等关系
	- pod里面又一个中间容器，叫Infra，它是Pod第一个建立起来的容器，pod里面其他可见的容器都通过加入到Infra容器的网络命名空间来实现共享
	- Infra容器一定要暂用尽量少的资源，它使用的镜像叫`k8s.gcr.io/pause`，一个永远处于暂停状态的镜像，解压之后也只有100-200KB
	- pod里可见容器都可以通过localhost通信，它们看到的网络设备跟Infra一致
- 凡是基于网络/调度/安全/存储相关的属性，基本都是pod级别

### pod yaml文件
- NodeSelector：将Pod和Node进行绑定的字段
- NodeNamwe：出现该字段，k8s认为pod已经被调度了

#### pod生命周期

#### 容器设计模式(pod)

[k8s日志收集方式](https://www.alibabacloud.com/blog/using-sidecar-mode-for-kubernetes-log-collection_594173)


Dynamic Admission Control
Dynamic Provisioning
<!--stackedit_data:
eyJoaXN0b3J5IjpbNTIwNjI2NjgsLTU0NTg1MDE4LC0xNzM0Nj
cxNzU5LDE5OTMzMTIwNzUsLTE3NzI3MjA2LC01MzA5Nzc1NzAs
LTE5MTc5MzI2NDMsMTYxMjQwMzAyLC0xMzMzMzM2MDk2LDEwOD
Q3ODUwMzIsODUxNzM3MTE4LDEwMzU1MjA2NzgsODY4NDg1NzI0
LDE1MDgyMjY0NTgsLTIwMDM2OTAzOTUsMTE2ODY1NzIwNCwxNj
kyMzIyMzksLTMzMzgxNjY5MSwtMTgzODc0NTQzMSwtMTkzNDEw
NTYxN119
-->