
# 容器
## 背景
- 容器技术的兴起源于 **PaaS** 技术的普及
- Docker 公司发布对docker项目具有里程碑式意义
- docker项目通过“容器镜像”，解决了**应用打包**的根本难题
- 容器本身没有价值，有价值的是**容器编排**
- 容器是一个**单进程** ------ 只有一个进程是可控的

## 容器是一种沙盒技术
### 沙盒边界的定义
- 代码二进制+数据二进制 -> 程序 -> **代码的可执行镜像**
	-> **进程的静态表现**
- 程序运行 -> 内存里的数据+寄存器的值+堆栈指令+被打开的文件+各种设备的状态信息的**集合** 
	-> **进程的动态表现**
- 容器的核心技术：通过约束和修改进程的动态表现，从而为其创造出**边界**

- Cgroups用来约束进程的动态表现
- Namespace用来修改进程的视图(障眼法)

-> **容器只是运行在宿主机上的一个特殊进程**

### 隔离与限制的理解
进一步理解 **容器只是运行在宿主机上的一个特殊进程**
- 隔离
	- 容器的软件实体是依赖于宿主机的
	- Linux内核中有很多资源和对象是不能被namespace隔离化的，最典型的例子就是时间
	- 共享宿主机内核使得容器暴露出的攻击面大，一般不会把容器服务直接暴露在公网中
	- [Namespace](https://lwn.net/Articles/531114/)
		- 容器使用的六种Namespace: 
			- Mount Namespace(挂载点)
			- UTS Namespace(主机名与域名)
			- IPC Namespace(信号量，消息队列和共享内存)
			- PID Namespace
			- Network Namespace
			- User Namespace
		- 对应六个系统调用参数：
			- CLONE_NEWNS
			- CLONE_NEWUTS
			- CLONE_NEWIPC
			- CLONE_NEWPID
			- CLONE_NEWNET
			- CLONE_NEWUSER
		- Mount Namespace改变的是容器进程对文件系统**挂载点**的认知，只有进行挂载操作之后，才能改变容器进程对文件系统的视图认知
- 限制（cgroups）
	- 早期被cgroup限制的进程组也被称为容器 
	- cgroups可以做：
		- 资源限制
		- 进程优先级设置和审计
		- 进程挂起和恢复
	- cgroups给用户暴露出来的操作接口是通过文件系统，是子系统目录加一组资源文件的组合
	- 对于Docker等容器项目只需要在每个子系统下，为每个容器创建一个资源控制组，然后在启动容器进程后，把这个进程加入该资源控制组

### 容器镜像
- 即使开启了Mount Namespace，容器看到的文件系统和宿主机一致
- 挂载在容器根目录上，用来为容器进程隔离后执行环境的文件系统，就是所谓的容器镜像，他有一个更专业的名字叫“rootfs”，根文件系统。常见的根文件系统通常包括/bin, /etc, /proc等
- rootfs只包括操作系统的外壳，不包括操作系统的灵魂，即内核，容器与宿主机共享内核，内核参数是全剧参数，牵一发而动全身
- 由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，它就意味着，应用以及运行所需要的依赖全部封装在一起，保证了**一致性**。
- 如果每次修改image都重新制作一次rootfs，会影响合作开发流程，因为不同应用的需求不一样。docker引入了增量修改的概念，引入了**层**的概念，用户的每一步操作，都是一个**层**，一个**增量rootfs**，基于联合文件系统操作实现的
- copy-on-write: 修改只读层的文件内容时，先复制到读写层后修改
whiteout: 删除只读层里的文件x时，会在读写层创建.wh.x。联合挂载的时候，x文件就会被.wh.x隐藏遮盖
init层: 只对当前容器有效的设置，且只读，比如hostname

### 创建容器
1. 启用Linux Namespace设置
2. 设置指定Cgroups参数
3. 切换进程的根目录

### 容器逃逸问题
- 危险挂载
	- 比如挂载了`/var/run/docker.sock`，容器里面就可以下载一个docker client，然后启动一个容器并挂载
- 
- 内核漏洞

## Docker子命令
![docker命令](https://img-blog.csdn.net/20180629115036543?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Fubm90YXRpb25feWFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

docker commit: 发生在宿主机namespace的

docker exec: 启动进程并进入容器进程所在的namespace中

volume: 在启用namespace之后，在chroot之前，将hostpath挂载到容器可读写层目录上；由于此时已经启动mount namespace，在宿主机上看到的该目录为空，因而commit也不会提交该目录下的内容
### 实验
#### Cgroups
1. 基本操作指示
	- 操作接口的文件系统位置: ```/sys/fs/cgroups```
	- 查看cgroup操作接口: ```mount -t cgroup```, 每个目录都是一个子系统
		```
		nina@nina-VirtualBox:~$ mount -t cgroup
		cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)
		cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
		cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
		cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
		cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
		cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
		cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
		cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
		cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
		cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
		cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
		cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)
		```
		
	- 查看每个资源可以被限制的方法: ```ls /sys/fs/cgroup/xxx```
		```
		nina@nina-VirtualBox:~$ ls /sys/fs/cgroup/memory
		cgroup.clone_children       memory.kmem.max_usage_in_bytes      memory.max_usage_in_bytes        memory.usage_in_bytes
		cgroup.event_control        memory.kmem.slabinfo                memory.move_charge_at_immigrate  memory.use_hierarchy
		cgroup.procs                memory.kmem.tcp.failcnt             memory.numa_stat                 notify_on_release
		cgroup.sane_behavior        memory.kmem.tcp.limit_in_bytes      memory.oom_control               release_agent
		memory.failcnt              memory.kmem.tcp.max_usage_in_bytes  memory.pressure_level            tasks
		memory.force_empty          memory.kmem.tcp.usage_in_bytes      memory.soft_limit_in_bytes
		memory.kmem.failcnt         memory.kmem.usage_in_bytes          memory.stat
		memory.kmem.limit_in_bytes  memory.limit_in_bytes               memory.swappiness
		```
	- Cgroup每一项子系统都有各自的功能
		- blkio限制设备I/O
		- memory限制内存资源
		- cpuset限制cpu核和对应的内存节点
		- cpu限制cpu资源
		- ...
	- 本实验基础: ```/sys/fs/cgroup/cpu/cfs_period```和```/sys/fs/cgroup/cfs_quota```可以限制进程在长度为cfs_period的时间段内，只能被分配到总量为cfs_quota的cpu时间

2. 限制一个进程的资源
	- 创建一个死循环进程，不加以限制
	  
	  创建进程
	  ```
	  nina@nina-VirtualBox:~$ while : ; do : ; done &
	  [1] 29536
	  ```
	  
	  查看进程资源
	  ```
	  nina@nina-VirtualBox:~$ top
	  top - 17:13:16 up 13 min,  2 users,  load average: 0.84, 0.77, 0.57
	  任务: 159 total,   2 running, 122 sleeping,   0 stopped,   0 zombie
	  %Cpu(s): 99.6 us,  0.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.4 si,  0.0 st
	  KiB Mem :  4039508 total,   823168 free,   449052 used,  2767288 buff/cache
	  KiB Swap:   483800 total,   483800 free,        0 used.  3301680 avail Mem
	  进程 USER      PR  NI    VIRT    RES    SHR � %CPU %MEM     TIME+ COMMAND
	  29536 nina      20   0   31412   2780   1340 R 99.3  0.1   1:26.09 bash
	  ```
	 - 创建一个新的cpu资源控制组
	   
	   创建cpu控制组
		```
		nina@nina-VirtualBox:/sys/fs/cgroup/cpu$ sudo mkdir cgroup_test
		[sudo] nina 的密码：
		nina@nina-VirtualBox:/sys/fs/cgroup/cpu$ ls cgroup_test/
		cgroup.clone_children  cpu.shares     cpuacct.usage_all          cpuacct.usage_sys
		cgroup.procs           cpu.stat       cpuacct.usage_percpu       cpuacct.usage_user
		cpu.cfs_period_us      cpuacct.stat   cpuacct.usage_percpu_sys   notify_on_release
		cpu.cfs_quota_us       cpuacct.usage  cpuacct.usage_percpu_user  tasks
		```

		查看cfs_period和cfs_quota
		```
		nina@nina-VirtualBox:/sys/fs/cgroup/cpu$ cat cgroup_test/cpu.cfs_period_us
		100000
		nina@nina-VirtualBox:/sys/fs/cgroup/cpu$ cat cgroup_test/cpu.cfs_quota_us
		-1
		```
	  - 修改控制组的cfs_period和cfs_quota的值
	   
	    向cgroup_test控制组的cfs_quota文件中写入20ms(20000um), 这意味着被该进程组限制的进程，每100ms的cpu运行时间只能使用20ms，cpu带宽为20%
	    ```echo 20000 > cgroup_test/cpu.cfs_quota_us```
	    
	    把死循环进程id写入```cgroup_test/task```
	    ```echo 29536 > cgroup_test/tasks```

		查看进程状态
		```
		nina@nina-VirtualBox:~$ top
		top - 17:37:57 up 38 min,  2 users,  load average: 0.61, 0.80, 0.67
		任务: 161 total,   2 running, 124 sleeping,   0 stopped,   0 zombie
		%Cpu(s): 16.7 us,  0.3 sy,  0.0 ni, 82.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
		KiB Mem :  4039508 total,   829832 free,   440136 used,  2769540 buff/cache
		KiB Swap:   483800 total,   483800 free,        0 used.  3310592 avail Mem
		进程 USER      PR  NI    VIRT    RES    SHR � %CPU %MEM     TIME+ COMMAND
		29536 nina      20   0   31412   2648   1208 R 20.5  0.1   8:51.85 bash
		```
3. 创建一个docker容器
	创建容器并设置cfs-period和cfs-quota
	```
	docker run --rm -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash
	```
	查看容器id
	```
	nina@nina-VirtualBox:~$ docker ps -a
	CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES
	a51bbb4e99a0        ubuntu              "/bin/bash"         8 seconds ago       Up 6 seconds                                    affectionate_cori
	```
	查看对应的cgroup
	```
	nina@nina-VirtualBox:/sys/fs/cgroup/cpu/docker/a51bbb4e99a0bc230452b6b8bf463a0bd241c6c0cf56870f76c86708b4a308f4$ cat cpu.cfs_period_us
	100000
	nina@nina-VirtualBox:/sys/fs/cgroup/cpu/docker/a51bbb4e99a0bc230452b6b8bf463a0bd241c6c0cf56870f76c86708b4a308f4$ cat cpu.cfs_quota_us
	20000
	```
	问题:
	宿主机的资源
	```
	nina@nina-VirtualBox:~$ top
	top - 22:51:13 up  4:07,  3 users,  load average: 0.04, 0.01, 0.00
	任务: 218 total,   1 running, 184 sleeping,   0 stopped,   0 zombie
	%Cpu(s):  1.3 us,  0.7 sy,  0.0 ni, 98.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
	KiB Mem :  4037392 total,  1005316 free,  1184284 used,  1847792 buff/cache
	KiB Swap:   483800 total,   483800 free,        0 used.  2604064 avail Mem
	```
	容器中的资源
	```
	root@a51bbb4e99a0:/#  top
	top - 14:52:33 up  4:09,  0 users,  load average: 0.04, 0.02, 0.00
	Tasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie
	%Cpu(s):  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
	KiB Mem :  4037392 total,  1005316 free,  1184276 used,  1847800 buff/cache
	KiB Swap:   483800 total,   483800 free,        0 used.  2604064 avail Mem
	```
	容器中内存的total和宿主机一致，这会带来一些问题。比如，给容器分配4G内存，容器跑的应用使用默认参数，此时默认参数会根据容器中内存的大小（宿主机内存大小）分配默认内存，那么此时默认值可能是大于4G，从而造成OOM
	    
#### Namespace
Namespace的API包括clone(), setns(), unshare()
- clone在创建新进程的同时创建新的namespace
- setns加入一个已经存在的namespace
- unshare会在原先的进程上进行namespace隔离

```
#define _GNU_SOURCE
#include <sys/types.h>
#include <sys/wait.h>
#include <stdio.h>
#include <unistd.h>
#include <signal.h>
#include <sched.h>

#define STACK_SIZE (1024*1024)

static char child_stack[STACK_SIZE];
char* const child_args[] = {
        "/bin/bash",
        NULL
};

int child_main() {
        printf("in child process, process id is %d\n", getpid());
        execv(child_args[0], child_args);
        printf("execv replaced current process, this will not be run!");
        return 1;
}

int main() {
        printf("Parent process is starting, process id is %d\n", getpid());
        int child_pid = clone(child_main, child_stack+STACK_SIZE, SIGCHLD, NULL);
        waitpid(child_pid, NULL, 0); // wait for child process exiting
        printf("Child process exited, current is parent process, process id is %d\n", getpid());
        return 0;
}
```

#### 文件系统实验
chroot filesystem command

mount -t aufs -o dirs=./d_A:./d_B none ./d_C

docker image inspect ubuntu:latest

cd /var/lib/docker/overlay2

docker inspect --format '{{ .State.Pid }}' container_id



# Kubenetes
## Why Kubenetes
### 目标
- 集群管理
	管理云平台中多个主机上的容器化的应用，Kubernetes的目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes提供了应用部署，规划，更新，维护的一种机制
- 调度
	过去很多集群管理项目(Swarm, mesos, yarn等)所擅长的就是把一个容器按照某种规则，放置在某个最佳节点上运行起来
- 编排
	kubenetes是按照用户的意愿和整个系统的规则，完全自动化的处理好容器之间的关系
### 特性
- 开源
- Golang项目
- API设计
- 分布式

### 设计理念
容错性/易扩展性
- API对象
	- 管理操作单元
	- K8s中所有的配置都是通过API对象的spec去设置的，也就是用户通过配置系统的理想状态来改变系统
	- 所有的操作都是声明式（Declarative）的而不是命令式（Imperative）的。声明式操作在分布式系统中的好处是稳定，不怕丢操作或运行多次，例如设置副本数为3的操作运行多次也还是一个结果，而给副本数加1的操作就不是声明式的，运行多次结果就错了。
- Pod
	- kubenetes运行部署应用的最小单位
	- Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务
	- 目前K8s中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）
- Node
	- pod运行所在的工作主机
	- 可以是物理机也可以是虚拟机
- Namespace
	- 虚拟的隔离作用
	- K8s集群初始有两个Namespace，分别是默认default和系统kube-system

## Kubenetes结构
这个容器编排集群结构需要有哪些组件呢？
- 调度器
- 编排器（控制器）
- API操作接口
- API对象存储

### Master/Worker Node结构图
![Kubenetes架构](https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.2/docs/design/architecture.png)
![Master Node](https://feisky.gitbooks.io/kubernetes/architecture/images/14791969222306.png)
![Worker Node](https://feisky.gitbooks.io/kubernetes/architecture/images/14791969311297.png)
### Master Node组件
- kube-apiserver
提供Kubenetes项目的操作接口，即kubenetes控制面板的前端，提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制
- kube-controller-manager
控制器管理组件，一般来说，一种类型的controller应该有自己单独的进程， 但为了减少复杂性，统一用单一进程管理。负责维护集群的状态，比如故障检测、自动扩展、滚动更新等
controller类型包含：
	node controller: 检测node状态
	replication controller: 保证replica对象的pod数量
	endpoints controller:加入service/pods？
	service account & token controller:为新的namespace创建合适的role和API的token
- etcd
保存了整个集群的状态，一致性和高可靠性键值存储设备，需要考虑备份恢复
- kube-scheduler
监测出没有被分配node的pod状态，并考虑资源需求(affinity specification)来分配node运行pod，负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上

### Worker Node组件
- kubelete
根据podspec文件保证pod中每个container能正常按需运行，负责维护容器的生命周期，同时也负责Volume（CVI）和网络（CNI）的管理
- kube-proxy
网络代理，负责为Service提供cluster内部的服务发现和负载均衡
- container runtime
用于运行container的，比如docker/containerd/cri-o，负责镜像管理以及Pod和容器的真正运行（CRI）

### [Addons](https://kubernetes.io/docs/concepts/cluster-administration/addons/)
用k8s的资源(deployment/daemon set等)去实现cluster的特性，一般都放在kube-system命名空间里
DNS：cluster必要的dns server

```
kubectl get po -n kube-system # 查看各个组件的pod
```
## API对象
### API对象元素
- 元数据metadata
	- 标识API对象的，每个对象都至少有3个元数据：namespace，name和uid
	- labels：用来标识和匹配不同的对象，例如用户可以用标签env来标识区分不同的服务部署环境，分别用env=dev、env=testing、env=production来标识开发、测试、生产的不同服务。通过selector来过滤labels
	- annotations：Label主要用于选择对象，可以挑选出满足特定条件的对象。相比之下，annotation 不能用于标识及选择对象。annotation中的元数据可多可少，可以是结构化的或非结构化的，也可以包含label中不允许出现的字符。
	- taint和toleration
- 规范spec
	- 描述了用户期望K8s集群中的分布式系统达到的理想状态（Desired State）（例如用户可以通过复制控制器Replication Controller设置期望的Pod副本数为3）
- 状态status
	- 描述了系统实际当前达到的状态（Status）（例如系统当前实际的Pod副本数为2；那么复制控制器当前的程序逻辑就是自动启动新的Pod，争取达到副本数为3）
[API Group](https://www.cnblogs.com/yuxiaoba/p/9803284.html)
[kubenetes openapi spec](https://raw.githubusercontent.com/kubernetes/kubernetes/v1.6.0/api/openapi-spec/swagger.json)
### kubectl
[https://kubectl.docs.kubernetes.io/](https://kubectl.docs.kubernetes.io/)
[https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/object-management/](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/object-management/)
#### 命令式命令
```sh
kubectl run nginx --image nginx
```
```sh
kubectl create deployment nginx --image nginx
```
#### 命令式配置对象
- kubectl 命令指定操作（创建，替换等），可选标志和至少一个文件名。指定的文件必须包含 YAML 或 JSON 格式的对象的完整定义。
- `replace`  命令式命令将现有规范替换为新提供的规范，并删除对配置文件中缺少的对象的所有更改。此方法不应与规范独立于配置文件进行更新的资源类型一起使用。比如类型为  `LoadBalancer`  的服务，它的  `externalIPs`  字段就是独立于集群的配置进行更新。
```sh
kubectl create -f nginx.yaml
```
```sh
kubectl delete -f nginx.yaml -f redis.yaml
```
```sh
kubectl replace -f nginx.yaml
```
#### 声明式配置对象

```sh
kubectl diff -R -f configs/
kubectl apply -R -f configs/
```
#### 对比声明式和命令式
- kube-apiserver在相应命令式请求的时候，一次只能处理一个写请求，否则会有可能产生冲突，而对于声明式请求，一次能处理多个写请求，而且具备merge能力
- 声明性对象配置更好地支持对目录进行操作并自动检测每个对象的操作类型（创建，修补，删除）
- 声明式对象配置难于调试并且出现异常时难以理解，使用差异的部分更新会创建复杂的合并和补丁操作
```
kubectl api-resources --namespaced=true
kubectl get pods --show-labels
```
## pod - Kubenetes原子调度单位
- 容器本质是进程，pod是一组容器(进程)
	- 很多时候一个应用需要多个进程相互协作，因而需要pod，一个容器里面也可以有多个应用，但利用pod可以解耦
	- pod只是一个逻辑概念，kubenetes真正操作的还是宿主机的namespace和cgroup
	- 可以把pod看作传统环境里的虚拟机，容器看作运行在这个机器里面的用户用户程序
- pod里面所有的容器都共享同一个network namespace，并且可以声明共享同一个volume 
	- `docker run --net=B --volumes-from=B --name=A image-A` 这样也可以实现共享，但是B一定先起起来，A和B之间是拓扑关系，不是对等关系
	- pod里面又一个中间容器，叫Infra，它是Pod第一个建立起来的容器，pod里面其他可见的容器都通过加入到Infra容器的网络命名空间来实现共享
	- Infra容器一定要暂用尽量少的资源，它使用的镜像叫`k8s.gcr.io/pause`，一个永远处于暂停状态的镜像，解压之后也只有100-200KB
	- pod里可见容器都可以通过localhost通信，它们看到的网络设备跟Infra一致，一个pod只有一个IP地址，它的所有网络资源都是里面所有容器共享的，所有容器的进出流量都是由Infra操作的，如果需要开发一个网络插件，应当考虑如何配置pod的network namespace，而不是可见容器的
	- pod的生命周期只与Infra容器有关，与其他可见容器无关
- Pod是一种编排思想，而不是具体的技术方案，可以用虚拟机实现pod，然后运行相应的容器
- Kubenetes项目中凡是基于网络/调度/安全/存储相关的属性，基本都是pod级别。凡是跟Linux Namespace相关的属性也一定是属于pod级别的，pod的设计，就是希望里面的容器尽可能的共享Linux Namespace，仅保留必要的隔离和限制能力
- Init容器
	- Init 容器总是运行到成功完成为止。
	- 每个 Init 容器都必须在下一个 Init 容器启动之前成功完成。

### pod yaml文件
TODO: yaml字段解释
[Pod配置文件](https://www.cnblogs.com/FRESHMANS/p/8444214.html)
- spec.NodeSelector：将Pod和Node进行绑定的字段
- spec.NodeName：出现该字段，k8s认为pod已经被调度了
- spec.shareProcessNamespace=true: 表示共享PID namespace
	- TODO: 比较spec.shareProcessNamesapce和spec.hostPIC
	- entrypoint和cmd的区别

Init Container
spec.container
### pod生命周期
#### postStart 和 preStop
```
apiVersion: v1
kind: Pod
metadata:
	name: lifecycle-demo
	namespace: k8s_experiments
spec:
	containers:
		- name: lifecycle-demo-container
		  image: nginx
		  # pull image的策略
		  imagePullPolicy: IfNotPresent
		  # container lifecycle hooks
		  lifecycle:
		  # 容器启动后，立即执行的操作
		  # 在Docker entrypoint之后，但不严格保证执行顺序，有可能entrypoint还没有执行结束
		  # 如果执行失败，pod也失败
			postStart:
			exec:
			command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
			# 容器杀死之前执行
			# 同步，只有执行完了之后才会杀死容器
			preStop:
			exec:
			command: ["/usr/sbin/nginx","-s","quit"]

# kubectl create -f pod_lifecycle.yaml
# kubectl attatch -it lifecycle-demo
```
#### pod状态
记录pod状态的字段：`pod.status.phase` `pod.status.conditions`
查看Pod Status：
`kubectl get pod pod_name --template="{{.status.phase}}"`
`kubectl describe pod pod_name`
`kubectl get pod pod_name --template="{{.status.conditions}}"`
##### `pod.status.phase`具体的状态类型：
- Pending: pod已经将yaml文件提交给kubenetes了，API对象已经被创建并且保存在etcd里面，但pod里面的容器因为某种原因不能被顺利创建，比如调度不成功
- Running：pod已经成功调度，并且与一个节点绑定，它包含的容器都已经被成功创建，并且至少有一个正在运行
- Succeeded: pod里面的容器都成功创建了，并且已经退出了，常见于依从性Job
- Failed：Pod里面的容器至少有一个以不正常状态（非0）退出，需要debug容器，查看容器日志
- Unknown：Pod的状态不能持续被kubelet获取汇报给kube-apiserver，可能是主从节点之间的通信出现了问题
##### `pod.status.conditions`具体类型
- PodScheduled
- Ready
- Initialized
- ContainersReady
```
kubectl get pod pod_name --template="{{.status.conditions}}"

[map[lastProbeTime:<nil> lastTransitionTime:2019-10-08T11:11:21Z status:True type:Initialized] map[lastProbeTime:<nil> lastTransitionTime:2019-10-08T11:11:22Z status:True type:Ready] map[lastProbeTime:<nil> lastTransitionTime:2019-10-08T11:11:22Z status:True type:ContainersReady] map[lastProbeTime:<nil> lastTransitionTime:2019-10-08T11:11:19Z status:True type:PodScheduled]]~
```
#### pod已经是Running状态，但实际不能提供服务
- 程序本身有bug，本来返回200，结果返回500
- 内存问题，程序已经僵死，但进程还在，无响应
- Dockerfile不规范，应用程序不是主程序，无法发现应用程序的问题
- 程序死循环
### Pod健康检查和恢复机制
- 健康检查：你可以手动为pod定义probe探针进行健康检查 `spec.contianers.livenessProbe`, kubenetes通过这个probe的返回值来决定容器的状态，而不是通过容器是否运行来决定
- Kubenetes没有docker的stop概念，恢复机制实际上是重新生产新的容器 `spec.restartPolicy`，默认值为Always
- pod的恢复机制永远发生在同一节点上，除非`spec.node`字段被修改
- restartPolicy：Always/Never/OnFailure
	- 一次性Job结束后，不需要重启，设置为OnFailure即可
	- 如果想要debug容器，可以使用Never，可以查看容器退出后的上下文环境
	- restartPolicy会影响pod的生命周期，只有pod里的所有容器有问题后，pod的状态才会从Running变成Failure，再加上restartPolicy，一般pod的Status都是Running状态。只有当restartPolicy=Never且所有容器异常时，Status才会变成Failed
	- `readnessProbe`是通过检查Pod能不能被Service访问，不会影响pod的生命周期
### Pod自动填充字段 - PodPreset
```
apiVersion: settings.k8s.io/v1alpha1
kind: PodPreset
metadata:
	name: allow-database
spec:
	selector:
		matchLabels:
			role: frontend
	env:
		- name: DB_PORT
		  value: "6379"
	volumeMounts:
		- mountPath: /cache
		  name: cache-volume
	volumes:
		- name: cache-volume
		  emptyDir: {}
---

apiVersion: v1
kind: Pod
metadata:
	name: website
	labels:
		app: website
		role: frontend
spec:
	containers:
		- name: website
		  image: nginx
		  ports:
			- containerPort: 80
```
https://kubernetes.io/docs/concepts/workloads/pods/podpreset/#enable-pod-preset

### Pod容器设计模式
![pod cheet sheet](https://jimmysong.io/kubernetes-handbook/images/kubernetes-pod-cheatsheet.png)

论文：[Design patterns for container-based distributed systems](https://www.usenix.org/system/files/conference/hotcloud16/hotcloud16_burns.pdf) ([design pattern](https://www.usenix.org/conference/hotcloud16/workshop-program/presentation/burns))
sidecar
[https://www.cnblogs.com/zhenyuyaodidiao/p/6514907.html](https://www.cnblogs.com/zhenyuyaodidiao/p/6514907.html)

[k8s日志收集方式](https://www.alibabacloud.com/blog/using-sidecar-mode-for-kubernetes-log-collection_594173)



## Projected Volume投射数据卷
- 为容器预先定义好的数据
- 分类
	- Secret
	- ConfigMap
	- Downward API
	- ServiceAccountToken
- 可以通过环境变量让容器获取这些数据，但是不具有自动更新功能，所以一般通过volume挂载方式获取数据
### Secret
- 把Pod想要访问的加密数据，存放到etcd中，然后通过pod容器里的volume挂载方式，访问到这些保存的加密数据
- 由Kubelet组件维护这些secrets
```
$ cat ./username.txt
admin
$ cat ./password.txt
c1oudc0w!

$ kubectl create secret generic user --from-file=./username.txt
$ kubectl create secret generic pass --from-file=./password.txt
```
```
apiVersion: v1
kind: Secret
metadata:
	name: mysecret
type: Opaque
data:
	user: YWRtaW4=
	pass: MWYyZDFlMmU2N2Rm
```
### ConfigMap
- 类似于Secret，保存不需要加密的配置文件信息
```
# .properties 文件的内容
$ cat example/ui.properties
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice

# 从.properties 文件创建 ConfigMap
$ kubectl create configmap ui-config --from-file=example/ui.properties

# 查看这个 ConfigMap 里保存的信息 (data)
$ kubectl get configmaps ui-config -o yaml
apiVersion: v1
data:
  ui.properties: |
    color.good=purple
    color.bad=yellow
    allow.textmode=true
    how.nice.to.look=fairlyNice
kind: ConfigMap
metadata:
  name: ui-config
  ...
```
### Downward API
- 让pod里面的容器可以直接获取pod API对象的信息
- 支持的字段，一定是pod里容器进程启动前可以获取的字段
```
1. 使用 fieldRef 可以声明使用:
spec.nodeName - 宿主机名字
status.hostIP - 宿主机 IP
metadata.name - Pod 的名字
metadata.namespace - Pod 的 Namespace
status.podIP - Pod 的 IP
spec.serviceAccountName - Pod 的 Service Account 的名字
metadata.uid - Pod 的 UID
metadata.labels['<KEY>'] - 指定 <KEY> 的 Label 值
metadata.annotations['<KEY>'] - 指定 <KEY> 的 Annotation 值
metadata.labels - Pod 的所有 Label
metadata.annotations - Pod 的所有 Annotation

2. 使用 resourceFieldRef 可以声明使用:
容器的 CPU limit
容器的 CPU request
容器的 memory limit
容器的 memory request
```
### ServiceAccountToken
- 应用场景：在容器里面安装一个k8s client用于获取k8s的API对象了，需要解决APIServer授权问题
- Service Account是K8s里面一种“服务账号”，它是k8s进行权限分配的对象，比如对API的操作权限，可读/可写
- 一种特殊的Secret对象
- 任何运行在kubenetes上的应用，都必须使用ServiceAccountToken的授权信息，才能访问API信息
- Kubenetes会提供一个默认的Service Account，无需显示挂载。在每个pod创建的时候，kubenetes自动在spec.volumes上添加的service account信息，对用户透明。可以通过`kubectl describe pod pod_name`查看container的mount信息查看
	```
	Mounts:
	      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zwgl5 (ro)
	```
- 应用程序加上了default service account授权信息后，就可以访问kubenetes API对象了，如果使用官方的kubenetes client（k8s.io/client-go）,可以自动加载这些授权文件，不用任何配置和编码操作，这种授权方式叫做InClusterConfig


## 控制器
- Pod是Kubenetes最基本的API对象，实际上就是对容器的进一步抽象和包装
- Kubenetes要编排的对象就是Pod
- 容器好比一个四面光秃秃的沙盒，Kubenetes这个吊车没有办法调度沙盒，Pod就是对容器进行了组合，并加上一些属性字段，相当于加上了吊环，供Kubentes调度
- 这是一个由API对象控制另一个API对象的循环控制操作
- 控制器都放在了pkg/controller目录下，它们都遵循通用编排模式，即**循环控制**
	```
	for {
	  实际状态 := 获取集群中对象 X 的实际状态（Actual State）
	  期望状态 := 获取集群中对象 X 的期望状态（Desired State）
	  if 实际状态 == 期望状态{
	    什么都不做
	  } else {
	    执行编排动作，将实际状态调整为期望状态
	  }
	}
	```
- 控制器由Master Node上的controller-manager来完成
- PodTemplate: 用deployment的spec.template来统一定义控制器所管理的pod
- 在所有API对象的metadata里，都有个字段叫做ownerReference，用于保存当前API对象拥有者的信息
### Deployment和ReplicaSet
```
##### 控制器定义
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  #### 被控制对象的模版
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```
```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-set
  labels:
    app: nginx
spec:
  replicas: 3  #副本数目
  selector:
    matchLabels:
      app: nginx
  template:   # 被控制pod模版
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
```
- RelicaSet的定义是deployment定义的子集，deployment控制器实际控制的是replicaset
- 由于是replicaset来控制pod的个数，那么deployment的restartPolicy为Always才有意义
- `kubectl get deployment`可以查看状态字段DESIRED/CURRENT(running pod)/UP-TO-DATE(updated pod)/AVAILABLE(running and updated pod)

#### Deployment控制器实现逻辑
- Deployment控制器从Etcd中获取所有标签为"app:nginx"的pod，然后统计它们的数量，这就是**实际状态**
- Deployment对象的spec.replicas字段就是**期望状态**
- 比较实际状态和期望状态，作出相应的操作（这个操作叫调协Reconcile Loop/Sync Loop）
#### 更新Deployment中的pod模版
- 如果更新了deployment的pod模版，这个时候deployment遵循一种**滚动更新(rolling update)**的方式来升级pod，由ReplicaSet来完成
- ReplicaSet由副本数目的定义和pod模版组成
- Deployment操作的对象是ReplicaSet而不是Pod，**层层递进**的关系，ReplicaSet保证系统中的pod个数永远等于指定个数，deployment控制操作replicaset的个数和属性，进而实现**水平扩展/收缩**和**滚动更新**这两个操作
	- 水平扩展/收缩:`kubectl scale deployment nginx-deployment --replicas=4`
	- 滚动更新：
		```
		kubectl create -f nginx-deployment.yaml --record   # 创建deployment，并用--record参数记录每次的操作
		kubectl get deployment   # 查看deployment当前的状态
		kubectl rollout status deployment/nginx-deployment #查看deployment控制的实时状态的变化
		kubectl edit deployment/nginx-deployment # 更新pod模版，保存退出后，立即执行滚动更新
		kubectl describe deployment nginx-deployment # 查看Events中滚动更新的流程，发现deployment会创建一个新的replicaset，交替逐一升级每一个pod，先创建一个新的，再删一个旧的
		```
- deployment中有个字段RollingUpdateStrategy，其中maxSurge指定除了DESIRED数量之外，在一次滚动更新中，deployment还可以创建多少个新的pod，maxUnavailable指定可以删多少个旧pod，可以用百分比表示
#### deployment实现发布模式
- 金丝雀部署/灰度部署
- 蓝绿/红黑/AB部署
	- 有两个相同的环境并行部署，一个当前运行的环境(蓝)，接收所有的用户流量，另一个是它的副本(绿)，闲置，部署新的版本，一旦通过测试，将流量接入副本环境，副本环境变成新的生产环境，如果有问题，再将流量切回原来的环境
	- 两个系统使用相同的持久化层和数据库后端
	- 蓝绿部署依赖流量路由。这可以通过更新主机的 DNS CNAMES 来完成，或者，你可以改变负载均衡的配置，让变更立即生效。类似 ELB 的连接特性可以用来提供无缝连接。
	- 你可以在生产环境的基础设施中小范围的部署新的应用代码。一旦应用签署发布，只有少数用户被路由到它。最大限度的降低影响。如果没有错误发生，新版本可以逐渐推广到整个基础设施
	- 对硬件的要求是两套系统
[k8s deployment strategies](https://github.com/ContainerSolutions/k8s-deployment-strategies)

### StatefulSet
- deployment的pod之间相互独立，可以随时杀死。但实际应用场景中，会存在多个实例之间的依赖关系，比如主从关系，主备关系，还有就是数据存储实例，应用实例和数据实例之间的关系，在数据实例的失败重建后会消失，从而导致应用失败
- StatefulSet 这个控制器的主要作用之一，就是使用 Pod 模板创建 Pod 的时候，对它们进行编号，并且按照编号顺序逐一完成创建工作。而当 StatefulSet 的“控制循环”发现 Pod 的“实际状态”与“期望状态”不一致，需要新建或者删除 Pod 进行“调谐”的时候，它会严格按照这些 Pod 编号的顺序，逐一完成这些操作。
- Statefulset直接管理的是pod
- Kubernetes 通过 Headless Service，为这些有编号的 Pod，在 DNS 服务器中生成带有同样编号的 DNS 记录
- StatefulSet 还为每一个 Pod 分配并创建一个同样编号的 PVC。这样，Kubernetes 就可以通过 Persistent Volume 机制为这个 PVC 绑定上对应的 PV，从而保证了每一个 Pod 都拥有一个独立的 Volume
- 拓扑状态
	- headless service
		- 它所代理的pod的ip多了一条对应的DNS记录
`<pod-name>.<svc-name>.<namespace>.svc.cluster.local`
- 存储状态
	- PV/PVC
- 应用案例：搭建MySQL集群
	- 是一个“主从复制”（Maser-Slave Replication）的 MySQL 集群
	- 有 1 个主节点（Master）
	- 有多个从节点（Slave）
	- 从节点需要能水平扩展
	- 所有的写操作，只能在主节点上执行
	- 读操作可以在所有节点上执行
- 

### DaemonSet
#### Pod 规范
- 这个 Pod 运行在 Kubernetes 集群里的每一个节点（Node）上
- 每个节点上只有一个这样的 Pod 实例
- 当有新的节点加入 Kubernetes 集群后，该 Pod 会自动地在新节点上被创建出来
- 而当旧节点被删除后，它上面的 Pod 也相应地会被回收掉
#### 应用
- 各种网络插件的 Agent 组件，都必须运行在每一个节点上，用来处理这个节点上的容器网络
- 各种存储插件的 Agent 组件，也必须运行在每一个节点上，用来在这个节点上挂载远程存储目录，操作容器的 Volume 目录
- 各种监控组件和日志组件，也必须运行在每一个节点上，负责这个节点上的监控信息和日志搜集。
### Job和Cronjob



## 服务发现
### 容器网络
Veth Pair
cat /sys/class/net/eth0/iflink
ip link
brctl show
route
iptables -L
bridge0
- 网络栈包括：网卡，回环设备，路由表，iptables
- 对于宿主机上的容器访问宿主机上其他容器/宿主机/其他机器，直接通过bridge二层数据链路层转发操作即可
- 对于节点A上的容器访问节点B上的容器，可以使用overlay network
- 容器间的“跨主通信”
	- Flannel
		- 这个 flannel0 设备的类型就比较有意思了：它是一个 TUN 设备（Tunnel 设备）。在 Linux 中，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能非常简单，即：在操作系统内核和用户应用程序之间传递 IP 包。以 flannel0 设备为例：像上面提到的情况，当操作系统将一个 IP 包发送给 flannel0 设备之后，flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 Flannel 进程。这是一个从内核态（Linux 操作系统）向用户态（Flannel 进程）的流动方向。反之，如果 Flannel 进程向 flannel0 设备发送了一个 IP 包，那么这个 IP 包就会出现在宿主机网络栈中，然后根据宿主机的路由表进行下一步处理。这是一个从用户态向内核态的流动方向。
	- Flannel三种后端实现
		- VXLAN（隧道模式）
			- Virtual Extensible LAN（虚拟可扩展局域网）
			- VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与“隧道”机制，构建出覆盖网络（Overlay Network）
			- VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。
		- UDP（隧道模式）
			- flanneld 在收到 container-1 发给 container-2 的 IP 包之后，就会把这个 IP 包直接封装在一个 UDP 包里，然后发送给 Node 2。不难理解，这个 UDP 包的源地址，就是 flanneld 所在的 Node 1 的地址，而目的地址，则是 container-2 所在的宿主机 Node 2 的地址。
			- 开销比较大，从container将数据发出宿主机，需要经过三次内核态用户态之间的拷贝
		- host-gw（pure三层网络方案）
			- host gateway模式，将宿主机当作gateway，设置访问Flannel子网的“下一跳”为子网所在的宿主机
			- calico也是这种host-gateway的思路，但是采用了BGP边界网关协议（node-to-node-mesh，复杂度会随着节点的数量的增加而增加）。在使用了 BGP 之后，你可以认为，在每个边界网关上都会运行着一个小程序，它们会将各自的路由表信息，通过 TCP 传输给其他的边界网关。而其他边界网关上的这个小程序，则会对收到的这些数据进行分析，然后将需要的信息添加到自己的路由表里。除了对路由信息的维护方式之外，Calico 项目与 Flannel 的 host-gw 模式的另一个不同之处，就是它不会在宿主机上创建任何网桥设备。Calico 的 CNI 插件会为每个容器设置一个 Veth Pair 设备，然后把其中的一端放置在宿主机上（它的名字以 cali 前缀开头）。最核心的“下一跳”路由规则，就是由 Calico 的 Felix 进程负责维护的。这些路由规则信息，则是通过 BGP Client 也就是 BIRD 组件，使用 BGP 协议传输而来的。Calico 项目实际上将集群里的所有节点，都当作是边界路由器来处理，它们一起组成了一个全连通的网络，互相之间通过 BGP 协议交换路由规则。这些节点，我们称为 BGP Peer。
			- calico的反射路由模式(大量节点的集群)：在路由上再包一层。calico支持kubenetes的networkPolicy对象，networkPolicy实际上就是转换成一些iptables
- 对比三层网络方案和隧道模式
	- 都是实现三层网络互通，都是通过对目的地址对MAC地址来操作进行的
	- 三层网络方案通过配置下一跳的路由信息来实现，隧道模式通过在IP包上加一层MAC包头来实现
	- 三层网络方案少了封包和解包的过程，性能更优，但需要自己想办法维护路由
	- 隧道模式简单，大部分工作都由内核完成，少了应用层面的工作量，缺点就是性能低
### Kubenetes网络
- Kubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的。
#### iptables
- 实际上，iptables 只是一个操作 Linux 内核 Netfilter 子系统的“界面”。顾名思义，Netfilter(防火墙) 子系统的作用，就是 Linux 内核里挡在“网卡”和“用户态进程”之间的一道“防火墙”。
- IP 包“一进一出”的两条路径上，有几个关键的“检查点”，它们正是 Netfilter 设置“防火墙”的地方。在 iptables 中，这些“检查点”被称为：链（Chain）。这是因为这些“检查点”对应的 iptables 规则，是按照定义顺序依次进行匹配的
- 当一个 IP 包通过网卡进入主机之后，它就进入了 Netfilter 定义的流入路径（Input Path）里。在这个路径中，IP 包要经过路由表路由来决定下一步的去向。而在这次路由之前，Netfilter 设置了一个名叫 PREROUTING 的“检查点”。在 Linux 内核的实现里，所谓“检查点”实际上就是内核网络协议栈代码里的 Hook（比如，在执行路由判断的代码之前，内核会先调用 PREROUTING 的 Hook）。
- Kubernetes 从底层的设计和实现上，更倾向于假设你已经有了一套完整的物理基础设施。然后，Kubernetes 负责在此基础上提供一种“弱多租户”（soft multi-tenancy）的能力。并且，基于上述思路，Kubernetes 将来也不大可能把 Namespace 变成一个具有实质意义的隔离机制，或者把它映射成为“子网”或者“租户”。毕竟你可以看到，NetworkPolicy 对象的描述能力，要比基于 Namespace 的划分丰富得多。这也是为什么，到目前为止，Kubernetes 项目在云计算生态里的定位，其实是基础设施与 PaaS 之间的中间层。这是非常符合“容器”这个本质上就是进程的抽象粒度的。

### Service
- kubenetes上的Service通过kube-proxy和iptables实现服务发现的
- 查看宿主机上的iptables： `iptables-save`
- 访问 Service VIP 的 IP 包经过上述 iptables 处理之后，就已经变成了访问具体某一个后端 Pod 的 IP 包了。不难理解，这些 Endpoints 对应的 iptables 规则，正是 kube-proxy 通过监听 Pod 的变化事件，在宿主机上生成并维护的。
- 一直以来，基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。Kubernetes 的 kube-proxy 还支持一种叫作 IPVS 的模式，IPVS 模式的工作原理，其实跟 iptables 模式类似。当我们创建了前面的 Service 之后，kube-proxy 首先会在宿主机上创建一个虚拟网卡（叫作：kube-ipvs0），并为它分配 Service VIP 作为 IP 地址。kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。而相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。
`-A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment "default/hostnames: cluster IP" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3`
凡是目的地址是10.0.1.175端口是80的IP包，都跳转到`KUBE-SVC-NWV5X2332I4OT4T3`的iptables链进行处理
```
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -j KUBE-SEP-57KPRZ3JQVENLNBR
```


```
-A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.3.6:9376

-A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.1.7:9376

-A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.2.3:9376
```
当您创建一个[服务](https://kubernetes.io/docs/user-guide/services)时，Kubernetes 会创建一个相应的[DNS 条目](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)。
该条目的形式是  `<service-name>.<namespace-name>.svc.cluster.local`， 这意味着如果容器只使用  `<service-name>`，它将被解析到本地命名空间的服务。 这对于跨多个命名空间（如开发、分级和生产）使用相同的配置非常有用。 如果您希望跨命名空间访问，则需要使用完全限定域名（FQDN）。
iptables
DNAT
SNAT

#### 三种模式
- ClusterIP+port
- LoadBalancer+nodePort
- External Name

#### Ingress
- Service的Service
- Kubenetes的反向代理
- Ingress工作在七层，Service工作在四层，负载均衡相关的配置只能在Ingress上做


Dynamic Admission Control
Dynamic Provisioning

## API Server
当发送一个rest请求给apiserver时：
1. 过滤请求，并进行一些前置性工作，比如授权，超时处理
2. 通过MUX/Route匹配找到API对应的类型定义
3. 创建对应的API对象，把YAML -> super version(字段全集)
4. 
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE0OTgwMDUzMzIsNDYwODYwNTkzLDkwMT
c5MTUxNiw4OTIzOTIyNjUsLTI1NjEyMzk0MSwtMTI5MDQ4MTUw
LDE2MzE4MzA3MjEsLTg3NzI4NDk3MiwtMTE3NDk5MDM3OSwxMz
g0MjQzNjg2LDkxODU2MzU1NCwxNjM3OTE2MTQ1LC0xMjIxMjY1
NDQ4LC0yMDg0OTc2NjQ4LC0yNzI1OTQ3NTUsMjA0NDk3NzQ4My
wtMTQxNzA5MjI3Niw5Mzc5NjUwODksNjk3ODg3MTQ3LC04OTgx
OTM0NTJdfQ==
-->